{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPRUDgVgbnZ7"
      },
      "outputs": [],
      "source": [
        "#Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/reddit/Train_Data.xls\")\n",
        "print(df.shape)\n",
        "\n",
        "#EDA\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "UHi-8doVcY2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "XGWDaKCPc14a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "OBshkolYc-YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns.values"
      ],
      "metadata": {
        "id": "FziyvZwmdw4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['text', 'author', 'controversiality', 'parent_text', 'parent_score', 'parent_votes', \n",
        "        'parent_author', 'parent_controversiality', 'Score']\n",
        "for col in cols:\n",
        "    print(col,':',df[col].nunique())"
      ],
      "metadata": {
        "id": "LoYsqH4LdneP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['parent_votes'], axis= 1, inplace=True)"
      ],
      "metadata": {
        "id": "s60F6OzSeA2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# handling categorical  data\n",
        "cat_cols = ['text','author','parent_text','parent_author']\n",
        "for col in df[cat_cols]:\n",
        "    df[col] = df[col].str.lower()\n",
        "    df[col] = df[col].str.strip()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Lf-QKtl2eks4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text pre processing\n",
        "# Removing punctuations\n",
        "\n",
        "for col in df[cat_cols]:\n",
        "    df[col] = df[col].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "vTd7j5GvfpOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization\n",
        "# Defining functions\n",
        "def text_tokens(row):\n",
        "    text = row['text']\n",
        "    tokens = word_tokenize(text)\n",
        "    token_words = [w for w in tokens if w.isalpha()]\n",
        "    return token_words\n",
        "df['text_tokens'] = df.apply(text_tokens, axis=1)\n",
        "\n",
        "def parent_text_tokens(row):\n",
        "    parent_text = row['parent_text']\n",
        "    tokens = word_tokenize(parent_text)\n",
        "    token_words = [w for w in tokens if w.isalpha()]\n",
        "    return token_words\n",
        "df['parent_text_tokens'] = df.apply(parent_text_tokens, axis=1)"
      ],
      "metadata": {
        "id": "XaOKr7XpQ8rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "38h22fqeSABA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "column_tokens = ['text_tokens','parent_text_tokens']\n",
        "\n",
        "for col in column_tokens:\n",
        "    df[col] = df[col].apply(lambda x: ' '.join([w for w in x if w not in (stop_words)]))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hBdK6o9BSZnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lemetizing\n",
        "wl = WordNetLemmatizer()\n",
        "for col in column_tokens:\n",
        "    df[col] = df[col].apply(lambda x: [wl.lemmatize(str(word)) for word in x.split()])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4aa6--1LT-H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text']= df['text_tokens'].apply(lambda x: ' '.join(x))\n",
        "df['parent_text']= df['parent_text_tokens'].apply(lambda x: ' '.join(x))\n",
        "df.drop(['text_tokens', 'parent_text_tokens'], axis=1, inplace= True)"
      ],
      "metadata": {
        "id": "C7i_fviIUa18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "7GJnRi6EUit5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing data\n",
        "cv = CountVectorizer()\n",
        "text_cv = cv.fit_transform(df['text']).toarray()\n",
        "text_cv = pd.DataFrame(text_cv, columns=cv.get_feature_names())\n",
        "\n",
        "tfidv = TfidfVectorizer(max_features=50,min_df=1,max_df=0.7)\n",
        "text_tf = tfidv.fit_transform(df['text']).toarray()\n",
        "text_tf = pd.DataFrame(text_tf, columns=tfidv.get_feature_names())\n"
      ],
      "metadata": {
        "id": "b0DtD0lSUspA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_cv.shape"
      ],
      "metadata": {
        "id": "j0z0mpeaVGqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_tf.shape"
      ],
      "metadata": {
        "id": "_rC5jQKAVakl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#numeric data\n",
        "numeric_data = df[['controversiality', 'parent_score', 'parent_controversiality','Score']]\n",
        "# Correlation of numerical features\n",
        "numeric_data.corr()"
      ],
      "metadata": {
        "id": "4ruRRrzDVzGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(numeric_data.corr() , annot = True)"
      ],
      "metadata": {
        "id": "wJqUitkFWgKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining features and Target\n",
        "X = numeric_data.drop('Score', axis=1)\n",
        "X = pd.concat([text_tf, X], axis=1)\n",
        "y = df['Score']"
      ],
      "metadata": {
        "id": "ncruMctMXQv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "BtZ6vYnQYRvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state = 7)"
      ],
      "metadata": {
        "id": "AIpH5EaYYvpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model building\n",
        "#Linear regression\n",
        "linear = LinearRegression()\n",
        "linear.fit(X_train, y_train)\n",
        "\n",
        "#Predicting values\n",
        "y_pred_linear = linear.predict(X_test)"
      ],
      "metadata": {
        "id": "CUK3GWT8Y6A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('RMSE for linear regression is: ', np.sqrt(mean_squared_error(y_test, y_pred_linear)))"
      ],
      "metadata": {
        "id": "sNQUE_4KZt_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN\n",
        "# Hyperparameter for n_neighbors\n",
        "param_grid = {'n_neighbors': np.arange(1, 100)}\n",
        "knn = KNeighborsRegressor()\n",
        "knn_cv = GridSearchCV(knn, param_grid, cv=10)\n",
        "knn_cv.fit(X_train, y_train)\n",
        "knn_cv.best_params_"
      ],
      "metadata": {
        "id": "hIaurHt6Z5Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cross validation showed us that 83 is the best value for n_neighbours\n",
        "knn = KNeighborsRegressor(n_neighbors = 83)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_knn = knn.predict(X_test)"
      ],
      "metadata": {
        "id": "9uSpovnuannt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('RMSE for KNN regression is: ', np.sqrt(mean_squared_error(y_test, y_pred_knn)))"
      ],
      "metadata": {
        "id": "LLUBKnzyaaRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "param_grid = {'n_estimators': np.arange(1, 50), 'max_depth': np.arange(1, 50)}\n",
        "Randomreg = RandomForestRegressor()\n",
        "Randomreg_cv = GridSearchCV(Randomreg, param_grid, cv=10)\n",
        "Randomreg_cv.fit(X_train, y_train)\n",
        "Randomreg_cv.best_params_"
      ],
      "metadata": {
        "id": "TCBjs258a36A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cross validation showed us\n",
        "Randomreg = RandomForestRegressor(n_estimators=9, max_depth=3, max_features='auto')\n",
        "Randomreg.fit(X_train, y_train)\n",
        "\n",
        "y_pred_random = Randomreg.predict(X_test)"
      ],
      "metadata": {
        "id": "U5xqn4uUbjBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}